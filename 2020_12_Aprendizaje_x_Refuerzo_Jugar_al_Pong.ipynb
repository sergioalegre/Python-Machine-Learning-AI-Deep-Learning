{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020 12 Aprendizaje x Refuerzo al Pong.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMwzTurnuRruZQzdl21Md6z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergioalegre/Python-for-AI-and-machine-learning/blob/master/2020_12_Aprendizaje_x_Refuerzo_Jugar_al_Pong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKVpZAnu9rdj"
      },
      "source": [
        "#Basado https://www.aprendemachinelearning.com/aprendizaje-por-refuerzo/#more-5911"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WYESawu9xiB"
      },
      "source": [
        "\"\"\"\r\n",
        "Simularemos el ambiente del juego y su compotamiento en la Jupyter Notebook.\r\n",
        "\r\n",
        "El agente será el “player 1” y sus acciones posible son 2:\r\n",
        "-mover hacia arriba\r\n",
        "-mover hacia abajo\r\n",
        "\r\n",
        "Las reglas del juego:\r\n",
        "-El agente tiene 3 vidas.\r\n",
        "-Si pierde… castigo, restamos 10 puntos.\r\n",
        "-Cada vez que le demos a la bola, recompensa, sumamos 10.\r\n",
        "-Para que no quede jugando por siempre, limitaremos el juego a 3000 iteraciones máximo ó alcanzar 1000 puntos y habremos ganado\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmWRzkR7_Qfd"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from random import randint\r\n",
        "from time import sleep\r\n",
        "from IPython.display import clear_output\r\n",
        "from math import ceil,floor\r\n",
        " \r\n",
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJPweDYv_VHi"
      },
      "source": [
        "\"\"\"\r\n",
        "Dentro de la clase Agente encontraremos la tabla donde iremos almacenando las políticas. En nuestro caso la tabla cuenta de 3 coordenadas:\r\n",
        "\r\n",
        "La posición actual del jugador.\r\n",
        "La posición “y” de la pelota.\r\n",
        "La posición en el eje “x” de la pelota.\r\n",
        "Además en esta clase, definiremos el factor de descuento, el learning rate y el ratio de exploración.\r\n",
        "\r\n",
        "Los métodos más importantes:\r\n",
        "\r\n",
        "get_next_step() decide la siguiente acción a tomar en base al ratio de exploración si tomar “el mejor paso” que tuviéramos almacenado ó tomar un paso al azar, dando posibilidad a explorar el ambiente\r\n",
        "update() aquí se actualizan las políticas mediante la ecuación de Bellman que vimos anteriormente. Es su implementación en python.\r\n",
        "\"\"\"\r\n",
        "class PongAgent:\r\n",
        "    \r\n",
        "    def __init__(self, game, policy=None, discount_factor = 0.1, learning_rate = 0.1, ratio_explotacion = 0.9):\r\n",
        "\r\n",
        "        # Creamos la tabla de politicas\r\n",
        "        if policy is not None:\r\n",
        "            self._q_table = policy\r\n",
        "        else:\r\n",
        "            position = list(game.positions_space.shape)\r\n",
        "            position.append(len(game.action_space))\r\n",
        "            self._q_table = np.zeros(position)\r\n",
        "        \r\n",
        "        self.discount_factor = discount_factor\r\n",
        "        self.learning_rate = learning_rate\r\n",
        "        self.ratio_explotacion = ratio_explotacion\r\n",
        "\r\n",
        "    def get_next_step(self, state, game):\r\n",
        "        \r\n",
        "        # Damos un paso aleatorio...\r\n",
        "        next_step = np.random.choice(list(game.action_space))\r\n",
        "        \r\n",
        "        # o tomaremos el mejor paso...\r\n",
        "        if np.random.uniform() <= self.ratio_explotacion:\r\n",
        "            # tomar el maximo\r\n",
        "            idx_action = np.random.choice(np.flatnonzero(\r\n",
        "                    self._q_table[state[0],state[1],state[2]] == self._q_table[state[0],state[1],state[2]].max()\r\n",
        "                ))\r\n",
        "            next_step = list(game.action_space)[idx_action]\r\n",
        "\r\n",
        "        return next_step\r\n",
        "\r\n",
        "    # actualizamos las politicas con las recompensas obtenidas\r\n",
        "    def update(self, game, old_state, action_taken, reward_action_taken, new_state, reached_end):\r\n",
        "        idx_action_taken =list(game.action_space).index(action_taken)\r\n",
        "\r\n",
        "        actual_q_value_options = self._q_table[old_state[0], old_state[1], old_state[2]]\r\n",
        "        actual_q_value = actual_q_value_options[idx_action_taken]\r\n",
        "\r\n",
        "        future_q_value_options = self._q_table[new_state[0], new_state[1], new_state[2]]\r\n",
        "        future_max_q_value = reward_action_taken  +  self.discount_factor*future_q_value_options.max()\r\n",
        "        if reached_end:\r\n",
        "            future_max_q_value = reward_action_taken #maximum reward\r\n",
        "\r\n",
        "        self._q_table[old_state[0], old_state[1], old_state[2], idx_action_taken] = actual_q_value + \\\r\n",
        "                                              self.learning_rate*(future_max_q_value -actual_q_value)\r\n",
        "    \r\n",
        "    def print_policy(self):\r\n",
        "        for row in np.round(self._q_table,1):\r\n",
        "            for column in row:\r\n",
        "                print('[', end='')\r\n",
        "                for value in column:\r\n",
        "                    print(str(value).zfill(5), end=' ')\r\n",
        "                print('] ', end='')\r\n",
        "            print('')\r\n",
        "            \r\n",
        "    def get_policy(self):\r\n",
        "        return self._q_table"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh8kqyxvCcvH"
      },
      "source": [
        "\"\"\"La clase Environment\r\n",
        "Encontramos implementada la lógica y control del juego del pong. Se controla que la pelotita rebote, que no se salga de la pantalla y se encuentran los métodos para graficar y animar en matplotlib.\r\n",
        "Por Defecto se define una pantalla de 40 pixeles x 50px de alto y si utilizamos la variable “movimiento_px = 5” nos quedará definida nuestra tabla de políticas en 8 de alto y 10 de ancho (por hacer 40/5=8 y 50/5=10). Estos valores se pueden modificar a gusto!\r\n",
        "Además, muy importante, tenemos el control de cuándo dar las recompensas y penalizaciones, al perder cada vida y detectar si el juego a terminado\"\"\"\r\n",
        "\r\n",
        "class PongEnvironment:\r\n",
        "    \r\n",
        "    def __init__(self, max_life=3, height_px = 40, width_px = 50, movimiento_px = 3):\r\n",
        "        \r\n",
        "        self.action_space = ['Arriba','Abajo']\r\n",
        "        \r\n",
        "        self._step_penalization = 0\r\n",
        "        \r\n",
        "        self.state = [0,0,0]\r\n",
        "        \r\n",
        "        self.total_reward = 0\r\n",
        "        \r\n",
        "        self.dx = movimiento_px\r\n",
        "        self.dy = movimiento_px\r\n",
        "        \r\n",
        "        filas = ceil(height_px/movimiento_px)\r\n",
        "        columnas = ceil(width_px/movimiento_px)\r\n",
        "        \r\n",
        "        self.positions_space = np.array([[[0 for z in range(columnas)] \r\n",
        "                                                  for y in range(filas)] \r\n",
        "                                                     for x in range(filas)])\r\n",
        "\r\n",
        "        self.lives = max_life\r\n",
        "        self.max_life=max_life\r\n",
        "        \r\n",
        "        self.x = randint(int(width_px/2), width_px) \r\n",
        "        self.y = randint(0, height_px-10)\r\n",
        "        \r\n",
        "        self.player_alto = int(height_px/4)\r\n",
        "\r\n",
        "        self.player1 = self.player_alto  # posic. inicial del player\r\n",
        "        \r\n",
        "        self.score = 0\r\n",
        "        \r\n",
        "        self.width_px = width_px\r\n",
        "        self.height_px = height_px\r\n",
        "        self.radio = 2.5\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        self.total_reward = 0\r\n",
        "        self.state = [0,0,0]\r\n",
        "        self.lives = self.max_life\r\n",
        "        self.score = 0\r\n",
        "        self.x = randint(int(self.width_px/2), self.width_px) \r\n",
        "        self.y = randint(0, self.height_px-10)\r\n",
        "        return self.state\r\n",
        "\r\n",
        "    def step(self, action, animate=False):\r\n",
        "        self._apply_action(action, animate)\r\n",
        "        done = self.lives <=0 # final\r\n",
        "        reward = self.score\r\n",
        "        reward += self._step_penalization\r\n",
        "        self.total_reward += reward\r\n",
        "        return self.state, reward , done\r\n",
        "\r\n",
        "    def _apply_action(self, action, animate=False):\r\n",
        "        \r\n",
        "        if action == \"Arriba\":\r\n",
        "            self.player1 += abs(self.dy)\r\n",
        "        elif action == \"Abajo\":\r\n",
        "            self.player1 -= abs(self.dy)\r\n",
        "            \r\n",
        "        self.avanza_player()\r\n",
        "\r\n",
        "        self.avanza_frame()\r\n",
        "\r\n",
        "        if animate:\r\n",
        "            clear_output(wait=True);\r\n",
        "            fig = self.dibujar_frame()\r\n",
        "            plt.show()\r\n",
        "\r\n",
        "        self.state = (floor(self.player1/abs(self.dy))-2, floor(self.y/abs(self.dy))-2, floor(self.x/abs(self.dx))-2)\r\n",
        "    \r\n",
        "    def detectaColision(self, ball_y, player_y):\r\n",
        "        if (player_y+self.player_alto >= (ball_y-self.radio)) and (player_y <= (ball_y+self.radio)):\r\n",
        "            return True\r\n",
        "        else:\r\n",
        "            return False\r\n",
        "    \r\n",
        "    def avanza_player(self):\r\n",
        "        if self.player1 + self.player_alto >= self.height_px:\r\n",
        "            self.player1 = self.height_px - self.player_alto\r\n",
        "        elif self.player1 <= -abs(self.dy):\r\n",
        "            self.player1 = -abs(self.dy)\r\n",
        "\r\n",
        "    def avanza_frame(self):\r\n",
        "        self.x += self.dx\r\n",
        "        self.y += self.dy\r\n",
        "        if self.x <= 3 or self.x > self.width_px:\r\n",
        "            self.dx = -self.dx\r\n",
        "            if self.x <= 3:\r\n",
        "                ret = self.detectaColision(self.y, self.player1)\r\n",
        "\r\n",
        "                if ret:\r\n",
        "                    self.score = 10\r\n",
        "                else:\r\n",
        "                    self.score = -10\r\n",
        "                    self.lives -= 1\r\n",
        "                    if self.lives>0:\r\n",
        "                        self.x = randint(int(self.width_px/2), self.width_px)\r\n",
        "                        self.y = randint(0, self.height_px-10)\r\n",
        "                        self.dx = abs(self.dx)\r\n",
        "                        self.dy = abs(self.dy)\r\n",
        "        else:\r\n",
        "            self.score = 0\r\n",
        "\r\n",
        "        if self.y < 0 or self.y > self.height_px:\r\n",
        "            self.dy = -self.dy\r\n",
        "\r\n",
        "    def dibujar_frame(self):\r\n",
        "        fig = plt.figure(figsize=(5, 4))\r\n",
        "        a1 = plt.gca()\r\n",
        "        circle = plt.Circle((self.x, self.y), self.radio, fc='slategray', ec=\"black\")\r\n",
        "        a1.set_ylim(-5, self.height_px+5)\r\n",
        "        a1.set_xlim(-5, self.width_px+5)\r\n",
        "\r\n",
        "        rectangle = plt.Rectangle((-5, self.player1), 5, self.player_alto, fc='gold', ec=\"none\")\r\n",
        "        a1.add_patch(circle);\r\n",
        "        a1.add_patch(rectangle)\r\n",
        "        #a1.set_yticklabels([]);a1.set_xticklabels([]);\r\n",
        "        plt.text(4, self.height_px, \"SCORE:\"+str(self.total_reward)+\"  LIFE:\"+str(self.lives), fontsize=12)\r\n",
        "        if self.lives <=0:\r\n",
        "            plt.text(10, self.height_px-14, \"GAME OVER\", fontsize=16)\r\n",
        "        elif self.total_reward >= 1000:\r\n",
        "            plt.text(10, self.height_px-14, \"YOU WIN!\", fontsize=16)\r\n",
        "        return fig"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPBkaiHrDAXA"
      },
      "source": [
        "\"\"\"\r\n",
        "El juego: Simular miles de veces para enseñar\r\n",
        "Finalmente definimos una función para jugar, donde indicamos la cantidad de veces que queremos iterar la simulación del juego e iremos almacenando algunas estadísticas sobre el comportamiento del agente, si mejora el puntaje con las iteraciones y el máximo puntaje alcanzado.\r\n",
        "\"\"\"\r\n",
        "def play(rounds=5000, max_life=3, discount_factor = 0.1, learning_rate = 0.1,\r\n",
        "         ratio_explotacion=0.9,learner=None, game=None, animate=False):\r\n",
        "\r\n",
        "    if game is None:\r\n",
        "        # si usamos movimiento_px = 5 creamos una tabla de politicas de 8x10\r\n",
        "        # si usamos movimiento_px = 3 la tabla sera de 14x17\r\n",
        "        game = PongEnvironment(max_life=max_life, movimiento_px = 3)\r\n",
        "        \r\n",
        "    if learner is None:\r\n",
        "        print(\"Begin new Train!\")\r\n",
        "        learner = PongAgent(game, discount_factor = discount_factor,learning_rate = learning_rate, ratio_explotacion= ratio_explotacion)\r\n",
        "\r\n",
        "    max_points= -9999\r\n",
        "    first_max_reached = 0\r\n",
        "    total_rw=0\r\n",
        "    steps=[]\r\n",
        "\r\n",
        "    for played_games in range(0, rounds):\r\n",
        "        state = game.reset()\r\n",
        "        reward, done = None, None\r\n",
        "        \r\n",
        "        itera=0\r\n",
        "        while (done != True) and (itera < 3000 and game.total_reward<=1000):\r\n",
        "            old_state = np.array(state)\r\n",
        "            next_action = learner.get_next_step(state, game)\r\n",
        "            state, reward, done = game.step(next_action, animate=animate)\r\n",
        "            if rounds > 1:\r\n",
        "                learner.update(game, old_state, next_action, reward, state, done)\r\n",
        "            itera+=1\r\n",
        "        \r\n",
        "        steps.append(itera)\r\n",
        "        \r\n",
        "        total_rw+=game.total_reward\r\n",
        "        if game.total_reward > max_points:\r\n",
        "            max_points=game.total_reward\r\n",
        "            first_max_reached = played_games\r\n",
        "        \r\n",
        "        if played_games %500==0 and played_games >1 and not animate:\r\n",
        "            print(\"-- Partidas[\", played_games, \"] Avg.Puntos[\", int(total_rw/played_games),\"]  AVG Steps[\", int(np.array(steps).mean()), \"] Max Score[\", max_points,\"]\")\r\n",
        "                \r\n",
        "    if played_games>1:\r\n",
        "        print('Partidas[',played_games,'] Avg.Puntos[',int(total_rw/played_games),'] Max score[', max_points,'] en partida[',first_max_reached,']')\r\n",
        "        \r\n",
        "    #learner.print_policy()\r\n",
        "    \r\n",
        "    return learner, game"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "inWxABN5DS4R",
        "outputId": "bda9fbae-7516-4d98-e73a-93aaa38e20a7"
      },
      "source": [
        "\"\"\"\r\n",
        "Para entrenar ejecutamos la función con los siguientes parámetros:\r\n",
        "- 6000 partidas jugará\r\n",
        "- ratio de explotación: el 85% de las veces será avaro, pero el 15% elige acciones aleatorias, dando lugar a la exploración.\r\n",
        "- learning rate = se suele dejar en el 10 por ciento como un valor razonable, dando lugar a las recompensas y permitiendo actualizar la importancia de cada acción poco a poco.\r\n",
        "- Tras más iteraciones, mayor importancia tendrá esa acción.\r\n",
        "- discount_factor = También se suele empezar con valor de 0.1 pero aquí utilizamos un valor del 0.2 para intentar indicar al algoritmo que nos interesa las recompensas a más largo plazo.\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "learner, game = play(rounds=5000, discount_factor = 0.2, learning_rate = 0.1, ratio_explotacion=0.85)\r\n",
        "\r\n",
        "\"\"\"Y vemos la salida del entreno, luego de unos 2 minutos:\"\"\""
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Begin new Train!\n",
            "-- Partidas[ 500 ] Avg.Puntos[ 14 ]  AVG Steps[ 217 ] Max Score[ 200 ]\n",
            "-- Partidas[ 1000 ] Avg.Puntos[ 22 ]  AVG Steps[ 245 ] Max Score[ 280 ]\n",
            "-- Partidas[ 1500 ] Avg.Puntos[ 25 ]  AVG Steps[ 256 ] Max Score[ 300 ]\n",
            "-- Partidas[ 2000 ] Avg.Puntos[ 26 ]  AVG Steps[ 260 ] Max Score[ 300 ]\n",
            "-- Partidas[ 2500 ] Avg.Puntos[ 27 ]  AVG Steps[ 264 ] Max Score[ 300 ]\n",
            "-- Partidas[ 3000 ] Avg.Puntos[ 30 ]  AVG Steps[ 273 ] Max Score[ 300 ]\n",
            "-- Partidas[ 3500 ] Avg.Puntos[ 32 ]  AVG Steps[ 279 ] Max Score[ 370 ]\n",
            "-- Partidas[ 4000 ] Avg.Puntos[ 34 ]  AVG Steps[ 286 ] Max Score[ 470 ]\n",
            "-- Partidas[ 4500 ] Avg.Puntos[ 35 ]  AVG Steps[ 289 ] Max Score[ 470 ]\n",
            "Partidas[ 4999 ] Avg.Puntos[ 36 ] Max score[ 470 ] en partida[ 3668 ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Y vemos la salida del entreno, luego de unos 2 minutos:'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "S6yHKGmmDk3f",
        "outputId": "09fcfe67-ed5a-4fa9-ba55-870960235a20"
      },
      "source": [
        "\"\"\"Veamos el resultado!\r\n",
        "Ya contamos con nuestro agente entrenado, ahora veamos qué tal se comporta en una partida de pong, y lo podemos ver jugar, pasando el parámetro animate=True.\r\n",
        "Antes de jugar, instanciamos un nuevo agente “learner2” que utilizará las políticas que creamos anteriormente.\r\n",
        "A este agente le seteamos el valor de explotación en 1, para evitar que tome pasos aleatorios.\"\"\"\r\n",
        "\r\n",
        "learner2 = PongAgent(game, policy=learner.get_policy())\r\n",
        "learner2.ratio_explotacion = 1.0  # con esto quitamos las elecciones aleatorias al jugar\r\n",
        "player = play(rounds=1, learner=learner2, game=game, animate=True)\r\n",
        "\r\n",
        "\"\"\"Y veremos nuestro juego de Pong en acción!\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}